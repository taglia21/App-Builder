# App-Builder Comprehensive Code Audit Report

**Date**: January 24, 2026  
**Severity Scale**: 游댮 Critical | 游 High | 游리 Medium | 游릭 Low

---

## Executive Summary

After systematically analyzing the entire App-Builder codebase, I've identified **47 issues** across 6 categories that explain why the system produces low-quality outputs. The most impactful problems are:

1. **LLM prompts that don't enforce structured output** - leading to unparseable responses
2. **Refinement engine is essentially disabled** - skips real validation  
3. **Code templates have syntax errors and missing imports**
4. **Intelligence gathering produces thin, unfiltered data**
5. **No validation of generated code quality**

---

## 游댮 CRITICAL BUGS (12 Issues)

### 1. [enhanced_engine.py#L430-L450](src/code_generation/enhanced_engine.py#L430-L450) - Generated Model Columns Have Wrong Syntax

**Bug**: The entity model template substitutes column definitions incorrectly.

```python
# Line 438-440
columns = []
for field_name, sql_type, py_type, required in entity['fields']:
    nullable = 'False' if required else 'True'
    columns.append(f'{field_name} = Column({sql_type}, nullable={nullable})')
```

**Problem**: `sql_type` like `String(255)` or `Text` isn't a valid SQLAlchemy type reference. Should be:
```python
columns.append(f"{field_name} = Column({sql_type}, nullable={nullable})")
# Actually generates: `name = Column(String(255), nullable=False)` 
# BUT String(255) needs to be imported and the syntax is wrong for some types
```

**Impact**: Generated models fail to compile. Apps won't start.

**Fix**: Create a proper type mapping and import generator.

---

### 2. [enhanced_engine.py#L162-L175](src/code_generation/enhanced_engine.py#L162-L175) - `_detect_features` LLM Response Not Validated

**Bug**: No validation of LLM JSON response structure.

```python
response = client.complete(prompt, json_mode=True)
return json.loads(response.content.replace('```json', '').replace('```', '').strip())
```

**Problem**: If LLM returns malformed JSON or wrong keys, the entire code generation fails silently or produces broken output.

**Impact**: Feature detection fails unpredictably, leading to incomplete apps.

**Fix**: Add Pydantic model validation:
```python
class FeatureFlags(BaseModel):
    needs_payments: bool = False
    needs_background_jobs: bool = False
    needs_ai_integration: bool = False
    needs_email: bool = False

try:
    data = json.loads(response.content...)
    return FeatureFlags(**data).model_dump()
except ValidationError:
    return FeatureFlags().model_dump()  # Safe defaults
```

---

### 3. [enhanced_engine.py#L178-L220](src/code_generation/enhanced_engine.py#L178-L220) - `_determine_core_entity` Returns Inconsistent Structure

**Bug**: LLM returns `fields` as list of lists, but fallback returns list of tuples.

```python
# LLM returns: [["field_name", "String(255)", "str", true], ...]
# Fallback returns: [('name', 'String(255)', 'str', True), ...]
```

**Problem**: Some downstream code expects tuples, some expects lists. Type inconsistency causes failures.

**Impact**: Entity model generation fails randomly.

**Fix**: Normalize to a consistent data structure using Pydantic.

---

### 4. [refinement/engine.py#L77-L110](src/refinement/engine.py#L77-L110) - Refinement Engine Skips All Real Checks

**Bug**: The refinement engine claims to run 5 checks but only `completeness` is implemented.

```python
def _run_all_checks(self, prompt: ProductPrompt) -> Dict[str, Dict[str, Any]]:
    return {
        "completeness": self._check_completeness(prompt),
        "consistency": {"passed": True, "issues": [], "severity": "none"},  # Simplified
        "technical_validity": {"passed": True, "issues": [], "severity": "none"},  # Simplified
        "security": {"passed": True, "issues": [], "severity": "none"},  # Simplified
        "feasibility": {"passed": True, "issues": [], "severity": "none"}  # Simplified
    }
```

**Impact**: Prompts with contradictions, security issues, and infeasible architectures pass through unchecked, producing garbage code.

**Fix**: Implement all checks using LLM-based validation.

---

### 5. [refinement/engine.py#L160-L175](src/refinement/engine.py#L160-L175) - `_fix_issues` Does Nothing

**Bug**: The issue fixer just returns the original prompt unchanged.

```python
def _fix_issues(self, prompt: ProductPrompt, check_results: Dict) -> ProductPrompt:
    # ... collects issues ...
    logger.info(f"Attempting to fix {len(all_issues)} issues")
    # For now, just return the original prompt
    return prompt  # <-- DOES NOTHING!
```

**Impact**: Even when issues are detected, they're never fixed. The refinement loop is useless.

---

### 6. [prompt_engineering/engine.py](src/prompt_engineering/engine.py) - Fallbacks Return Skeleton Data

**Bug**: All `_fallback_*` methods return nearly empty data structures.

```python
def _fallback_feature_requirements(self, idea: StartupIdea) -> Dict[str, Any]:
    return {
        "core_features": [
            {"id": "F-CORE-001", "name": "User Authentication", "priority": "P0-Critical"},
            # Only 3 generic features!
        ],
        ...
    }
```

**Impact**: When LLM fails (common), the product spec has minimal features, leading to skeleton apps.

**Fix**: Make fallbacks more robust with industry-specific defaults based on idea type.

---

### 7. [llm/client.py#L98-L112](src/llm/client.py#L98-L112) - Gemini JSON Mode Doesn't Actually Enforce JSON

**Bug**: For Gemini, `json_mode` only appends text to the prompt but doesn't use Gemini's native JSON mode.

```python
if json_mode:
    full_prompt += "\n\nRespond with valid JSON only. No other text or markdown."
# Gemini supports actual JSON mode: `response_mime_type="application/json"`
```

**Impact**: Gemini often returns markdown-wrapped JSON or explanatory text, breaking parsing.

**Fix**: Use Gemini's native JSON response format.

---

### 8. [idea_generation/engine.py#L75-L85](src/idea_generation/engine.py#L75-L85) - Startup Idea Names Are Generic Garbage

**Bug**: Idea names are generated from keywords without intelligence.

```python
keywords = pain_point.keywords[:2] if pain_point.keywords else ["Tool"]
name = f"{keywords[0].title()} Solution"  # Produces: "Invoice Solution", "Manual Solution"
```

**Impact**: All ideas have meaningless names like "Invoice Solution" or "Time Solution".

**Fix**: Use LLM to generate catchy, memorable product names.

---

### 9. [scoring/engine.py#L165-L175](src/scoring/engine.py#L165-L175) - Company Size Parsing Is Fragile

**Bug**: Enterprise scoring parses company size strings with substring matching.

```python
if "5000+" in company_size or "enterprise" in company_size.lower():
    score = 10
elif "1000" in company_size:  # Matches "1000" but also "10000"!
    score = 9
```

**Impact**: Scoring is inconsistent. "10000" matches "1000" case.

**Fix**: Parse company sizes properly with ranges.

---

### 10. [quality_assurance/engine.py](src/quality_assurance/engine.py) - QA Only Runs Formatters, No Validation

**Bug**: The QA engine only runs Black/Isort/Prettier - it doesn't validate:
- Python syntax (beyond what Black catches)
- TypeScript compilation
- Missing imports
- Runtime errors
- API contract mismatches

**Impact**: Syntactically formatted but logically broken code passes QA.

**Fix**: Add actual validation:
- Python: `ast.parse()` + `pyflakes`/`mypy`
- TypeScript: `tsc --noEmit`
- Contract validation: Compare backend schemas with frontend types

---

### 11. [enhanced_engine.py#L34-L35](src/code_generation/enhanced_engine.py#L34-L35) - Duplicate Import

**Bug**: Same import appears twice.

```python
from src.code_generation.frontend_templates import (
    ...
    FRONTEND_COMPONENTS_DATA_TABLE,
    FRONTEND_COMPONENTS_DATA_TABLE,  # DUPLICATE!
    ...
)
```

**Impact**: While Python handles this, it indicates copy-paste errors elsewhere.

---

### 12. [file_templates.py](src/code_generation/file_templates.py) - Generated Code Has Hardcoded Placeholders

**Bug**: Templates use `${variable}` but some aren't substituted.

```python
BACKEND_CONFIG_PY = '''...
DATABASE_URL: str = os.getenv(
    "DATABASE_URL", 
    "postgresql://postgres:postgres@db:5432/${db_name}"  # OK
)
...'''
```

This one is fine, but the worker_service substitution in docker-compose fails silently when empty.

---

## 游 HIGH SEVERITY ISSUES (10 Issues)

### 13. [intelligence/processor.py#L85-L100](src/intelligence/processor.py#L85-L100) - Pain Point Detection Is Keyword-Only

**Problem**: Pain points are identified by simple regex patterns.

```python
pain_indicators = [
    r"\bi wish\b",
    r"\bwhy (isn't|isnt|aren't|arent)\b",
    ...
]
```

**Impact**: Misses nuanced pain points, includes false positives. Many real complaints don't use these exact phrases.

**Fix**: Use LLM-based pain point classification with confidence scoring.

---

### 14. [intelligence/processor.py#L175-L200](src/intelligence/processor.py#L175-L200) - Industry Detection Is Naive

**Problem**: Industries are detected by keyword presence only.

```python
industry_keywords = {
    "software": ["software", "saas", "app", "platform"],
    # "real_estate" has space: "real estate" but posts might say "realestate"
}
```

**Impact**: Misclassifies industries, misses many, doesn't handle multi-industry posts.

---

### 15. [scoring/engine.py#L145-L160](src/scoring/engine.py#L145-L160) - Urgency Score Doesn't Validate Input

**Problem**: Divides by length without checking for empty list.

```python
avg_urgency = sum(pp.urgency_score for pp in related_pain_points) / len(related_pain_points)
```

Empty list causes ZeroDivisionError (though there's a guard above, the pattern is fragile).

---

### 16. [idea_generation/engine.py#L55-L65](src/idea_generation/engine.py#L55-L65) - Infinite Loop Risk in Idea Generation

**Problem**: While loop with retry limit, but if `ideas` stays empty, it still exits.

```python
while len(ideas) < self.min_ideas and retry_count < max_retries:
    retry_count += 1
    if ideas:  # Only generates variations if there ARE ideas
        extra_ideas = self._generate_variations(ideas[:5])
```

If initial generation produces 0 ideas, variations can't help. Better error handling needed.

---

### 17. [idea_generation/llm_engine.py#L90-L115](src/idea_generation/llm_engine.py#L90-L115) - JSON Parsing Is Fragile

**Problem**: Multiple hacky attempts to extract JSON from LLM response.

```python
start = response.find('[')
end = response.rfind(']') + 1
if start != -1 and end > start:
    json_str = response[start:end]
    return json.loads(json_str)
```

**Impact**: Fails on nested JSON, comments, or trailing text.

---

### 18. [prompt_engineering/engine.py](src/prompt_engineering/engine.py) - No Token Budget Management

**Problem**: Each section generation uses 2000-5000 tokens, but there's no tracking of total context or budget.

**Impact**: On cheaper models with small context windows, truncation happens silently.

---

### 19. [pipeline.py#L140-L145](src/pipeline.py#L140-L145) - Uses `product_prompt` Not `gold_standard_prompt` for Code Gen

**Problem**: After refinement, code generation uses the original `product_prompt`, not the refined version.

```python
# Step 6: Generate Code
codebase = self.code_generator.generate(product_prompt, output_dir, theme=self.theme)
# Should be: gold_standard_prompt.product_prompt or just pass gold_standard_prompt
```

**Impact**: Refinement improvements are thrown away!

---

### 20. [enhanced_engine.py#L420-L430](src/code_generation/enhanced_engine.py#L420-L430) - Payment Module Generated Without Validation

**Problem**: Dynamically generates Stripe module via LLM without any validation.

```python
prompt = "Write a production-ready Python FastAPI module using 'stripe' library..."
resp = client.complete(prompt)
payment_code = resp.content.replace('```python', '').replace('```', '')
self._write_file('backend/app/core/payment.py', payment_code, 'backend')
```

**Impact**: Potentially broken, insecure payment code is written directly to file.

---

### 21. [enhanced_engine.py#L300-L320](src/code_generation/enhanced_engine.py#L300-L320) - `_verify_and_fix` Loop Can Generate Worse Code

**Problem**: If LLM "fix" introduces new errors, the loop continues with progressively worse code.

```python
for attempt in range(3):
    try:
        ast.parse(content)
        return content
    except SyntaxError as e:
        content = self._fix_code_with_llm(content, str(e))  # Each fix might break more
```

No validation that the fix is actually better than the original.

---

### 22. No Rate Limiting on LLM Calls

**Problem**: Multiple engines call LLM without rate limiting or backoff coordination.

**Impact**: Hit rate limits, cause failures, waste API credits.

---

## 游리 MEDIUM SEVERITY ISSUES (15 Issues)

### 23. [config.py#L170-L175](src/config.py#L170-L175) - `_data_sources` Not Persisted Properly

The data sources are stored in a private attribute that might not serialize correctly.

---

### 24. [models.py#L27-L30](src/models.py#L27-L30) - PainPoint IDs Are Regenerated

```python
id: UUID = Field(default_factory=uuid4)
```

When pain points are loaded from demo data, new UUIDs are generated, breaking ID references.

---

### 25. Missing Error Handling in Data Sources

[intelligence/sources/reddit.py](src/intelligence/sources/reddit.py) - PRAW errors aren't handled gracefully.

---

### 26. [intelligence/engine.py#L100-L105](src/intelligence/engine.py#L100-L105) - Demo Data ID Conversion Loses References

```python
opp_dict_copy['pain_point_ids'] = []  # Just throws away the references!
```

---

### 27. Frontend Types Don't Match Backend Schemas

[enhanced_engine.py#L830-L880](src/code_generation/enhanced_engine.py#L830-L880) - `_generate_frontend_types` creates types from entity definition, but the type mapping is incomplete.

---

### 28. No Database Migrations Generated

Only `Base.metadata.create_all()` is used. No Alembic migration files are generated, making schema updates impossible.

---

### 29. [file_templates.py#L70-L75](src/code_generation/file_templates.py#L70-L75) - SECRET_KEY Warning Logic Runs at Import Time

The warning in Settings validator runs when the module is imported, not just when used.

---

### 30. No Test Data Seeding

Generated apps have no sample data, making demos difficult.

---

### 31. Frontend Has No API Client Generated

Components reference API endpoints but there's no generated Axios/fetch client with type safety.

---

### 32. [enhanced_engine.py#L600-L620](src/code_generation/enhanced_engine.py#L600-L620) - Deployment Providers Instantiated Without Config

```python
vercel = VercelProvider()
v_config = DeploymentConfig(provider=DeploymentProviderType.VERCEL, region="iad1")
vercel._generate_vercel_config(...)
```

Calling private methods of providers without proper initialization.

---

### 33. No .dockerignore Generated

Large node_modules and __pycache__ are included in Docker builds.

---

### 34. CORS Origins Default Includes Only Localhost

Production deployments will fail CORS until manually configured.

---

### 35. No Logging Configuration in Generated Apps

Basic print/logger statements but no structured logging setup.

---

### 36. [quality_assurance/engine.py#L75-L80](src/quality_assurance/engine.py#L75-L80) - Prettier Uses `shell=True`

Security risk and platform-specific behavior.

---

### 37. Generated README Has Placeholder `{db_name}`

Some template substitutions are missed in the README.

---

## 游릭 LOW SEVERITY ISSUES (10 Issues)

38. Inconsistent logging (loguru vs logging module)
39. No type hints on many internal methods
40. Magic strings throughout (should be constants/enums)
41. No input sanitization on LLM prompts (prompt injection possible)
42. Hardcoded subreddit list in Reddit source
43. No pagination handling in Reddit/GitHub collectors
44. Missing `__all__` exports in package `__init__.py` files
45. Tests don't cover LLM integration (mock only)
46. No caching of LLM responses for identical prompts
47. Version strings hardcoded instead of dynamic

---

## Root Cause Analysis

### Why Generated Apps Are "Garbage"

1. **Refinement is disabled** - The system claims to refine prompts but skips all real validation
2. **Fallbacks produce skeletons** - When LLM fails, fallbacks have 3 generic features instead of 8+
3. **No code validation** - QA only formats, doesn't check if code runs
4. **Entity modeling is broken** - SQL types aren't properly converted
5. **Idea quality is poor** - Names are keyword-concatenation, not creative

### Why The System Is Buggy

1. **Inconsistent data structures** - Lists vs tuples vs dicts across the pipeline
2. **No Pydantic validation on LLM responses** - Raw JSON parsing fails silently
3. **Exception handling swallows details** - Many `except Exception as e: logger.debug(e)`
4. **Race conditions in async code** - Multiple LLM calls without coordination

---

## Prioritized Fix List (By Impact)

### Phase 1: Critical Fixes (1-2 days)

| Priority | Issue | File | Impact |
|----------|-------|------|--------|
| 1 | Implement real refinement checks | refinement/engine.py | Prompts will be validated |
| 2 | Fix `_fix_issues` to actually fix | refinement/engine.py | Issues will be resolved |
| 3 | Fix entity column generation | enhanced_engine.py | Models will compile |
| 4 | Add Pydantic validation to LLM responses | All engines | Parsing won't break |
| 5 | Use gold_standard_prompt for code gen | pipeline.py | Refinement matters |

### Phase 2: Quality Improvements (3-5 days)

| Priority | Issue | File | Impact |
|----------|-------|------|--------|
| 6 | Add actual code validation in QA | quality_assurance/engine.py | Catch broken code |
| 7 | Implement LLM-based pain point classification | intelligence/processor.py | Better data quality |
| 8 | Generate meaningful idea names | idea_generation/engine.py | Better ideas |
| 9 | Improve fallback data quality | prompt_engineering/engine.py | Graceful degradation |
| 10 | Use Gemini native JSON mode | llm/client.py | Reliable parsing |

### Phase 3: Architecture Improvements (1-2 weeks)

| Priority | Issue | File | Impact |
|----------|-------|------|--------|
| 11 | Add rate limiting/retry coordination | llm/client.py | Stability |
| 12 | Generate Alembic migrations | enhanced_engine.py | Production-ready |
| 13 | Add frontend API client generation | enhanced_engine.py | Type safety |
| 14 | Implement test data seeding | enhanced_engine.py | Demo-ready apps |
| 15 | Add comprehensive E2E tests | tests/ | Reliability |

---

## Recommended Quick Wins

### 1. Enable Real Refinement (30 min fix)

```python
# In refinement/engine.py, replace simplified checks with:
def _check_consistency(self, prompt: ProductPrompt) -> Dict[str, Any]:
    content = json.loads(prompt.prompt_content)
    system_prompt = "Check this product spec for internal contradictions..."
    response = self.llm_client.complete(prompt=json.dumps(content), system_prompt=system_prompt, json_mode=True)
    return json.loads(response.content)
```

### 2. Fix Pipeline to Use Refined Prompt (5 min fix)

```python
# In pipeline.py line ~140, change:
codebase = self.code_generator.generate(product_prompt, output_dir, theme=self.theme)
# To:
final_prompt = gold_standard_prompt.product_prompt if gold_standard_prompt else product_prompt
codebase = self.code_generator.generate(final_prompt, output_dir, theme=self.theme)
```

### 3. Add Pydantic Validation to Entity (20 min fix)

```python
# Create a validated entity model:
class EntityField(BaseModel):
    name: str
    sql_type: str
    python_type: str
    required: bool

class EntityDefinition(BaseModel):
    name: str
    class_name: str = Field(alias="class")
    lower: str
    table: str
    fields: List[EntityField]
```

---

## Conclusion

The App-Builder has solid architectural bones but critical implementation gaps. The primary issue is that **quality gates are disabled or non-functional**:

1. Refinement doesn't refine
2. QA doesn't validate
3. LLM responses aren't validated
4. Fallbacks produce minimal content

Fixing the top 5 critical issues would dramatically improve output quality. The code generation templates are mostly correct - the problem is the data flowing into them.

